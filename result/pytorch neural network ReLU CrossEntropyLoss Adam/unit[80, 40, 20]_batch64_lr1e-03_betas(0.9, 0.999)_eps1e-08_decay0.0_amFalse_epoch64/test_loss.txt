# pytorch neural network ReLU CrossEntropyLoss Adam:
#  unit=[80, 40, 20],batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=64
0.70384936
0.33431507
0.32504775
0.31879992
0.31540566
0.30978612
0.30311187
0.30004241
0.29795270
0.29684950
0.29685669
0.29717582
0.29914272
0.30013312
0.30152697
0.30568229
0.30878478
0.31465014
0.31604047
