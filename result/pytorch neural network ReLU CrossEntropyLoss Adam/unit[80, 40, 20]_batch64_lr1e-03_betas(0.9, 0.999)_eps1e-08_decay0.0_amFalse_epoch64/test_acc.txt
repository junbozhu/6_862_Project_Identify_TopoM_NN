# pytorch neural network ReLU CrossEntropyLoss Adam:
#  unit=[80, 40, 20],batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=64
0.27867132
0.84720278
0.85804194
0.86153847
0.86433566
0.86783218
0.87062937
0.87622380
0.87622380
0.87552446
0.87622380
0.87692308
0.87587410
0.87447554
0.87587410
0.87307692
0.87132865
0.87062937
0.87167829
