# pytorch neural network ReLU CrossEntropyLoss Adam:
#  unit=[80, 20],batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=64
# train_loss, train_acc, val_loss, val_acc
0.02253308 0.14683308 0.02188378 0.14652984
0.01430396 0.01182580 0.01356237 0.00534869
0.00568186 0.00237953 0.01405918 0.00547158
0.00541365 0.00406697 0.01514534 0.00738013
0.00720495 0.00455910 0.01563143 0.00930328
0.00649224 0.00542158 0.01516468 0.00823651
0.00944075 0.00519054 0.01473551 0.00850415
0.00957828 0.00538076 0.01628911 0.00797435
0.00976393 0.00510963 0.01666794 0.00876748
0.00834596 0.00534016 0.01730023 0.00960435
0.00899962 0.00463916 0.01648599 0.00795541
0.00924828 0.00444522 0.01646095 0.00619094
0.00889438 0.00456947 0.01432381 0.00707147
0.00720896 0.00430859 0.01667549 0.00981506
0.00711142 0.00365380 0.01449996 0.00485476
0.00828238 0.00459779 0.01642795 0.00600688
0.00734679 0.00360732 0.01596039 0.00570969
0.00776225 0.00319787 0.01824590 0.00468944
0.00720618 0.00329643 0.01604709 0.00726200
0.00725870 0.00356700 0.01841438 0.00723534
0.00649242 0.00277160 0.01766698 0.00565852
0.00588746 0.00251764 0.01870745 0.00652328
0.00560607 0.00270826 0.01467107 0.00495202
0.00523125 0.00267188 0.01653551 0.00518720
0.00508239 0.00271665 0.01754570 0.00657098
0.00496632 0.00207118 0.01507453 0.00603726
0.00494315 0.00255062 0.01566073 0.00443129
0.00459486 0.00199106 0.01860752 0.00534372
0.00427807 0.00201183 0.01779193 0.00496799
0.00441098 0.00218048 0.01540759 0.00567717
0.00397394 0.00157315 0.01470402 0.00410384
0.00392328 0.00197751 0.01682233 0.00315783
0.00365210 0.00152076 0.01664105 0.00464103
0.00455236 0.00218230 0.01439169 0.00495910
0.00403733 0.00173209 0.01704012 0.00432780
0.00368709 0.00185231 0.01807003 0.00441057
0.00415055 0.00230393 0.01836641 0.00525898
0.00424955 0.00169293 0.01961857 0.00803858
0.00477383 0.00249523 0.01467916 0.00428139
0.00341958 0.00140751 0.01764942 0.00463037
0.00449811 0.00181377 0.01957685 0.00424342
0.00285821 0.00153162 0.01649439 0.00464352
0.00347861 0.00150425 0.01892867 0.00479326
0.00341662 0.00181278 0.01948746 0.00419008
0.00573890 0.00228662 0.02195201 0.00564527
0.00523867 0.00198790 0.02064563 0.00439153
0.00434483 0.00180312 0.01934201 0.00483984
0.00402204 0.00175118 0.02105756 0.00553752
0.00462777 0.00179289 0.02928995 0.00595708
0.00322964 0.00148700 0.02906227 0.00611363
0.00624111 0.00272055 0.02458700 0.00448233
0.00636349 0.00287551 0.02535857 0.00448941
0.00380496 0.00105536 0.02058845 0.00521356
0.00620035 0.00244098 0.02071647 0.00661573
0.00328467 0.00170089 0.02274436 0.00390148
0.00329692 0.00160365 0.01799082 0.00330726
0.00379066 0.00216180 0.02929076 0.00441270
0.00454106 0.00165686 0.02384641 0.00457025
0.00234183 0.00157556 0.02472287 0.00417749
0.00334247 0.00147023 0.02498844 0.00595485
0.00337598 0.00173363 0.02294691 0.00620626
0.00352539 0.00186641 0.01936973 0.00529574
0.00276500 0.00109135 0.02435743 0.00445220
0.00297436 0.00153031 0.02611063 0.00534337
0.00209596 0.00118692 0.02568146 0.00330538
