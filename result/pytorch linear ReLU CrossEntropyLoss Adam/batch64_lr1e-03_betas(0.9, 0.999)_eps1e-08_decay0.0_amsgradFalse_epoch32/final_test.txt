# pytorch linear ReLU CrossEntropyLoss Adam:
#  batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=32
#  training loss,	 training accuracy,	 test loss,	 test accuracy
0.67680343 0.59817439 0.67471872 0.60664338
0.50357106 0.75459313 0.41920143 0.79720283
0.41262045 0.80322391 0.38007689 0.82902098
0.39255540 0.81588656 0.36643076 0.83951050
0.38522653 0.82081956 0.36001581 0.84300697
0.38165456 0.82369393 0.35639447 0.84580421
0.37957322 0.82485920 0.35412599 0.84475523
0.37824079 0.82567489 0.35261194 0.84510487
0.37734153 0.82680130 0.35155623 0.84650350
0.37671203 0.82707322 0.35079560 0.84895104
0.37625814 0.82777238 0.35023338 0.84895104
0.37592249 0.82804430 0.34980901 0.84965032
0.37566879 0.82843268 0.34948292 0.84930068
0.37547330 0.82870460 0.34922834 0.84965032
0.37532011 0.82889879 0.34902669 0.85000002
0.37519825 0.82905418 0.34886484 0.84965032
0.37509998 0.82924837 0.34873332 0.85034966
0.37501978 0.82940376 0.34862524 0.85000002
0.37495359 0.82959801 0.34853549 0.85000002
0.37489841 0.82971454 0.34846024 0.85034966
0.37485198 0.82971454 0.34839659 0.85034966
