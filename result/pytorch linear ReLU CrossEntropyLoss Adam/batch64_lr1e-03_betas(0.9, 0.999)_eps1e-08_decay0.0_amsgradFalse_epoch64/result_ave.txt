# pytorch linear ReLU CrossEntropyLoss Adam:
#  batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=64
# train_loss, train_acc, val_loss, val_acc
0.69991746 0.50640617 0.69931558 0.51054642
0.46298025 0.76608140 0.40978096 0.80472872
0.39871000 0.81289701 0.39174201 0.81719279
0.38768009 0.81925452 0.38587392 0.82072585
0.38278961 0.82231457 0.38298294 0.82262849
0.37992804 0.82401508 0.38152035 0.82437556
0.37835480 0.82555156 0.38042222 0.82491940
0.37791119 0.82647954 0.37955179 0.82639501
0.37736422 0.82732114 0.37926264 0.82666693
0.37573430 0.82791244 0.37891633 0.82705513
0.37610133 0.82826202 0.37873243 0.82709413
0.37610794 0.82848647 0.37846721 0.82763769
0.37543615 0.82852100 0.37843545 0.82732696
0.37543880 0.82874543 0.37830376 0.82759914
0.37489392 0.82913387 0.37828964 0.82771516
0.37484696 0.82896556 0.37833962 0.82728840
0.37490060 0.82907347 0.37810639 0.82779334
0.37421174 0.82908638 0.37825074 0.82814282
0.37475541 0.82916840 0.37822199 0.82783215
0.37492412 0.82928925 0.37807896 0.82806526
0.37423449 0.82944030 0.37822542 0.82818154
0.37469920 0.82937988 0.37812375 0.82798734
0.37447467 0.82976401 0.37833743 0.82853091
0.37469051 0.82947052 0.37829413 0.82829809
0.37471317 0.82936261 0.37816972 0.82849218
0.37398730 0.82974674 0.37822104 0.82853109
0.37358777 0.82963883 0.37838293 0.82845336
0.37462313 0.82935397 0.37825652 0.82845337
0.37404216 0.82969925 0.37829251 0.82833691
0.37460687 0.82951798 0.37826329 0.82833681
0.37607442 0.82948776 0.37832370 0.82822055
0.37389501 0.82955682 0.37839070 0.82802634
0.37388625 0.82964746 0.37843523 0.82794880
0.37371481 0.82955252 0.37831567 0.82833692
0.37425775 0.82939281 0.37830927 0.82833673
0.37464608 0.82950501 0.37831773 0.82771589
0.37418314 0.82952660 0.37831056 0.82856965
0.37511793 0.82965606 0.37815319 0.82822018
0.37407047 0.82956545 0.37836345 0.82868621
0.37397608 0.82966904 0.37849962 0.82837555
0.37388213 0.82973812 0.37837138 0.82880257
0.37344743 0.82961292 0.37841372 0.82849228
0.37349506 0.82949207 0.37834400 0.82856974
0.37503322 0.82969062 0.37860664 0.82841437
0.37340060 0.82937557 0.37860910 0.82856983
0.37373338 0.82972515 0.37836497 0.82853092
0.37351342 0.82978987 0.37848113 0.82818153
0.37353283 0.82964748 0.37847947 0.82818136
0.37343257 0.82998842 0.37842449 0.82818154
0.37378571 0.82976399 0.37842352 0.82860875
0.37395369 0.82963020 0.37847743 0.82837600
0.37436932 0.82984598 0.37851625 0.82810400
0.37393852 0.82991073 0.37840992 0.82860855
0.37448151 0.82978989 0.37833702 0.82790980
0.37446312 0.82964314 0.37842762 0.82798743
0.37537390 0.82960860 0.37852955 0.82833681
0.37360225 0.82973811 0.37860179 0.82775461
0.37336420 0.82975969 0.37853555 0.82856983
0.37475520 0.82960861 0.37858046 0.82849201
0.37377382 0.82981579 0.37849018 0.82818153
0.37372670 0.82977262 0.37862208 0.82856974
0.37357533 0.82965177 0.37851717 0.82818145
0.37415630 0.82969494 0.37860263 0.82837564
0.37402434 0.82984600 0.37864702 0.82853091
0.37385801 0.82972947 0.37860611 0.82779334
