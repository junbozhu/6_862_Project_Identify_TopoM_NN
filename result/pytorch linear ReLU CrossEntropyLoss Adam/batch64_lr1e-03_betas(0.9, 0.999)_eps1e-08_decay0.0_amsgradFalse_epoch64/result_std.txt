# pytorch linear ReLU CrossEntropyLoss Adam:
#  batch=64,lr=1e-03,betas=(0.9, 0.999),eps=1e-08,decay=0.0,amsgrad=False,epoch=64
# train_loss, train_acc, val_loss, val_acc
0.02810023 0.10701643 0.02836620 0.10529520
0.01636279 0.00856336 0.01471939 0.00850561
0.00684221 0.00455679 0.01223009 0.00468825
0.00285967 0.00187253 0.01263747 0.00478272
0.00230749 0.00123015 0.01294608 0.00446835
0.00158220 0.00100290 0.01356601 0.00544134
0.00176859 0.00067163 0.01367009 0.00487292
0.00205807 0.00052565 0.01386613 0.00464034
0.00285520 0.00081868 0.01397746 0.00461862
0.00129844 0.00061985 0.01418900 0.00494651
0.00225760 0.00072615 0.01414415 0.00480369
0.00255033 0.00055262 0.01440495 0.00473037
0.00204796 0.00080862 0.01450701 0.00531231
0.00146903 0.00071200 0.01457583 0.00458560
0.00172257 0.00079789 0.01490258 0.00504475
0.00190578 0.00073394 0.01473883 0.00494024
0.00261742 0.00092899 0.01468355 0.00523534
0.00187395 0.00085445 0.01460208 0.00540204
0.00142000 0.00079021 0.01472389 0.00522686
0.00190049 0.00074278 0.01483989 0.00509879
0.00160057 0.00086282 0.01484826 0.00546640
0.00101947 0.00086293 0.01476674 0.00557906
0.00213435 0.00082364 0.01493644 0.00569033
0.00185960 0.00083536 0.01479987 0.00536289
0.00280647 0.00091589 0.01481364 0.00521406
0.00144577 0.00070110 0.01495731 0.00522043
0.00143088 0.00079078 0.01480029 0.00564334
0.00209769 0.00079165 0.01482502 0.00544770
0.00207209 0.00062495 0.01478179 0.00593576
0.00274670 0.00074995 0.01494689 0.00576852
0.00341490 0.00077242 0.01485827 0.00502679
0.00177652 0.00052763 0.01472077 0.00615798
0.00132096 0.00049198 0.01508093 0.00545667
0.00156661 0.00085267 0.01474880 0.00549270
0.00155945 0.00072739 0.01461501 0.00609389
0.00236995 0.00097245 0.01496915 0.00575238
0.00202294 0.00107718 0.01482183 0.00579585
0.00296655 0.00074494 0.01480847 0.00566455
0.00181593 0.00083046 0.01491654 0.00528390
0.00176292 0.00102819 0.01477778 0.00613494
0.00198095 0.00076547 0.01487667 0.00586262
0.00163441 0.00090253 0.01500728 0.00516162
0.00167606 0.00078464 0.01499758 0.00525570
0.00254222 0.00085007 0.01486506 0.00534903
0.00153093 0.00084526 0.01497534 0.00589608
0.00212799 0.00081496 0.01478220 0.00537715
0.00139126 0.00074569 0.01499753 0.00558908
0.00120574 0.00092540 0.01495204 0.00586311
0.00152395 0.00079376 0.01508934 0.00575907
0.00246827 0.00072243 0.01503189 0.00543041
0.00242806 0.00079144 0.01468891 0.00601774
0.00255790 0.00071453 0.01500354 0.00606379
0.00200285 0.00078266 0.01495641 0.00572256
0.00212316 0.00083073 0.01506580 0.00539095
0.00316921 0.00087038 0.01491858 0.00546152
0.00320724 0.00098237 0.01508190 0.00567900
0.00134046 0.00074027 0.01507723 0.00579233
0.00164858 0.00089937 0.01460355 0.00540538
0.00273201 0.00088141 0.01485231 0.00554222
0.00203913 0.00097171 0.01488103 0.00528418
0.00151283 0.00092133 0.01484165 0.00559997
0.00122415 0.00071080 0.01483962 0.00585780
0.00228714 0.00087308 0.01492833 0.00547541
0.00150714 0.00084495 0.01487672 0.00576922
0.00212265 0.00065914 0.01516376 0.00599736
